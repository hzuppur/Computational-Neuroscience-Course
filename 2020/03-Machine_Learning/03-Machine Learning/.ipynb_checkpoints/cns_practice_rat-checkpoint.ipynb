{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Introduction to Computational Neuroscience</center></h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Practice Session III: Machine Learning</center></h1>\n",
    "<center>Aqeel Labash, Daniel Majoral, Raul Vicente</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My **Pseudonym** is: <font color='green'>[YOUR ANSWER]</font> and it took me approximately: <font color='green'>[YOUR ANSWER]</font> hours to complete the home work.\n",
    "\n",
    "The data of how long it took you to complete the home work will help us to improve the home works to be balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before** you start you need to install an extra library. \n",
    "\n",
    "After you activate your conda environment `activate Py3_ICNS` or `source activate Py3_ICNS` execute `conda install scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The main purpose of the fields of neural encoding and neural decoding is to learn the relation\n",
    "between a stimulus and the neural response elicited by this stimulus. When studying encoding\n",
    "we want to predict how the brain would respond given a certain stimulus, we estimate the\n",
    "probability p(brainresponse|stimulus). In neural decoding we do the inverse - we read the brain\n",
    "activity using imaging techniques - and ask the question ”which stimulus caused this activity?”,\n",
    "we look for p(stimulus|brainresponse).</p>\n",
    "\n",
    "<p>In a decoding paradigm we present the test subjects with different stimuli while at the same\n",
    "time recording responses on their brains. The task is to create a model, which takes any piece of\n",
    "the recorded brain data as an input and predicts which stimulus was responsible for producing\n",
    "this piece of data.</p>\n",
    "\n",
    "<p>In the last practice session we kind of manually created one such model: by looking at\n",
    "the firings of 72 neurons we might pretty accurately guess the orientation of the bar on the\n",
    "screen. We would be able to predict which stimulus was shown, given the neural responses.</p>\n",
    "\n",
    "<p>Nevertheless, very often the data can be too massive or too complex to find such direct relation\n",
    "between activity and stimulus by simple observation. That’s where machine learning comes to\n",
    "our aid.</p>\n",
    "\n",
    "<p>The main goal of machine learning can be summarized as providing an automatic way of\n",
    "finding the dependencies between a set of features (neural data) and the corresponding labels\n",
    "(the stimuli).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue let us go through vocabulary:\n",
    "+ A dataset is a structure which contains all the data we have.\n",
    "+ A dataset consist of instances, often also referred to as data points.\n",
    "+ In case of classification, instances consist of features and a class label.\n",
    "+ Features are parameters, which describe our data. For example the average spiking rate\n",
    "of a neuron might be a feature, the power of EEG in a certain frequency band might be\n",
    "a feature, etc. Each instance has its own values for each of the features.\n",
    "+ All feature values of an instance put together form a feature vector.\n",
    "+ Feature vector is a representation of the instance in the feature space.\n",
    "+ Each instance belongs to a certain class - the name/ID number of stimulus that caused\n",
    "the features.\n",
    "+ The goal of a machine learning algorithm is to create a model, which can guess the class\n",
    "(classify) of the instance given only its feature vector. A good model should be able to\n",
    "1\n",
    "do this also on previously unseen feature vectors (generalize).\n",
    "+ The model is created from examples. Those are instances for which the class is known. A\n",
    "set of such examples is called training set, because we train our model on it.\n",
    "+ Test set is another set of instances, for which we also know the true class, but we do not\n",
    "share this knowledge with the model. Instead we ask the model to guess the class of each\n",
    "instance.\n",
    "+ We can then see how many instances from the test set model has identified correctly and\n",
    "the rate:  \n",
    "\n",
    "<center> $ \\dfrac{Number\\ of\\ correctly\\ classified\\ instances}{Total\\ number\\ of\\ instances} $   </center> \n",
    "\n",
    "is called accuracy and it is used to evaluate model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Where is the rat? (2.5pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we will try to predict in which part of space a rat is located based on its neural\n",
    "activity. When collecting the data some electrodes were inserted to the rat’s hippocampus\n",
    "where neurons responsible for navigation are located. At the same time the location of the rat\n",
    "was tracked by a camera. We have preprocessed the data so that:\n",
    "- 1. Features are spike counts in different neurons during a 500ms interval\n",
    "- 2. Class corresponds to which of the 16 areas the rat was located during these 500ms\n",
    "- 3. Your goal is to create a model that predicts the area based on the neural activity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/rat.png) <center> Figure 1: Find in which zone the rat is! </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the data folder you find the following files:\n",
    "   1. spikes `spike_counts.txt` - count of spikes form 71 neuron at each timestep\n",
    "   2. blocks `location_areas.txt` - region where the rat is at each timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the data and make sure you understand what is what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "%matplotlib notebook\n",
    "\n",
    "spike_counts = []\n",
    "f = open('data/spike_counts.txt', 'r')\n",
    "line = f.readline()\n",
    "while line != '':\n",
    "    results = line.strip().split(' ')\n",
    "    results = list(map(float, results))\n",
    "    spike_counts += [results]\n",
    "    line = f.readline()\n",
    "\n",
    "location_areas = genfromtxt('data/location_areas.txt', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Randomly divide the instances into training and test sets, so that 80 percent of the data is in training set (You can use `sklearn.model_selection.train_test_split` from scikit-learn library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10105\n",
      "2527\n",
      "10105\n",
      "2527\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(spike_counts, train_size=0.8 , random_state=42)\n",
    "Y_train, Y_test = train_test_split(location_areas, train_size=0.8 , random_state=42)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(Y_train))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use LDA (linear discriminant analysis) as the classifier (you can use `sklearn.discriminant_analysis.LinearDiscriminantAnalysis` from `scikit-learn` library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Learn a model using the examples in the training set. Predict the locations for instances\n",
    "in test set and compare them with the true locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly guessed 37% of the time\n"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "correctCount = 0\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    prediction = clf.predict([X_test[i]])[0]\n",
    "    test = Y_test[i]\n",
    "    if prediction == test:\n",
    "        correctCount += 1\n",
    "    ##print(\"The predicted result is: {} and the real result is: {}\".format(prediction, test))\n",
    "    \n",
    "print(\"Correctly guessed {}% of the time\".format(round(correctCount/len(X_test) * 100), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10. 10. 13. ...  8.  7.  6.]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How much is the overall accuracy in the test set? How much is the precision for each class\n",
    "separately (return 16 values)?\n",
    "precision of class i =\n",
    "points in zone i correctly classif ied as i\n",
    "total number of points classif ied as in zone i\n",
    "\n",
    "**HINT**: class accuracies can be calculated by dividing the diagonal values of confusion\n",
    "matrix with the column sums.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 32.89,\n",
       " 2: 43.31,\n",
       " 3: 23.95,\n",
       " 4: 43.59,\n",
       " 5: 26.72,\n",
       " 6: 34.67,\n",
       " 7: 57.83,\n",
       " 8: 32.95,\n",
       " 9: 25.0,\n",
       " 10: 24.86,\n",
       " 11: 34.29,\n",
       " 12: 30.38,\n",
       " 13: 29.63,\n",
       " 14: 31.79,\n",
       " 15: 41.45,\n",
       " 16: 36.0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code\n",
    "class_accuracy = {}\n",
    "class_sum = {}\n",
    "class_precentage = {}\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    prediction = clf.predict([X_test[i]])[0]\n",
    "    test = Y_test[i]\n",
    "    \n",
    "    if test == prediction:\n",
    "        if test in class_accuracy.keys():\n",
    "            class_accuracy[test] = class_accuracy[test] +1\n",
    "        else:\n",
    "            class_accuracy[test] = 1\n",
    "            \n",
    "    if test in class_sum.keys():\n",
    "        class_sum[test] = class_sum[test] +1\n",
    "    else:\n",
    "        class_sum[test] = 1\n",
    "\n",
    "for i in range(1,17):\n",
    "    class_precentage[i] = round(class_accuracy[i] / class_sum[i] * 100, 2)\n",
    "class_precentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Draw a confusion matrix. What additional information (beyond class accuracies) does this matrix\n",
    "provide? (you can use `sklearn.metrics.confusion_matrix`)\n",
    "\n",
    "You need to produce a confusion matrix, but it does not need to be a nice colourful drawing. It can also be just a readable (!!) printout of the matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-d4c6198f2496>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Your code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Your code\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "confusion_matrix(clf.predict(X_test), Y_test)\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in \"ABCDEFGHIJK\"],\n",
    "                  columns = [i for i in \"ABCDEFGHIJK\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise 1.2: A few questions to keep in mind (1.5pt) (answer in more than 1 phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 : Why do you need to separate training and test datasets, that is why can’t you evaluate\n",
    "your model on the training set? What does it mean if you accuracies on training set and test\n",
    "set are very different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Your Answer</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 : If our model in the previous excercise would just predict class labels completely randomly , what would be the\n",
    "average prediction accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Your Answer</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3 : If our model in the previous excercise would always predict the class label that is the most common label in our\n",
    "dataset, what would the average accuracy be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Your Answer</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Which picture was shown? (2pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will work with fMRI dataset3 by Haxby et al.4. As you may recall, fMRI measures the\n",
    "blood oxygen level in the brain with high spatial precision. The data was recorded while test subject was\n",
    "presented with images from 9 categories:  \n",
    "\n",
    "(1) house     \n",
    "(2) scrambled   \n",
    "(3) cat   \n",
    "(4) shoe   \n",
    "(5) bottle   \n",
    "(6) scissors   \n",
    "(7) chair   \n",
    "(8) face   \n",
    "(9) something else.  \n",
    "\n",
    "You can see them (except for the “something else” category) on the Figure: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/stimuli.png) <center> Figure 2: Examples of stimuli. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we have is already preprocessed ([1](https://openfmri.org/dataset/ds000105)), so instead of ≈25000 voxels in the whole brain we only\n",
    "use 577 voxels from relevant brain areas. In machine learning terminology this means that each instance has\n",
    "577 features and belongs to one of the 9 classes. You will find the feature data in `data/voxels.txt` and class\n",
    "information `data/labels.txt`. First one is 1452×577 matrix (1452 instances 577 features each) and the second\n",
    "one is a vector of length 1452 (each instance has a class). The question we want to answer is: Is it possible to\n",
    "decode from the fMRI signal the image the test subject was looking at ?. Your task will once again b to build a predictive model.\n",
    "Perform the following steps and report the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the data. (Look at it. Always look at the data)\n",
    "2. Split data into training and test set, 80% for traing and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train a LDA (Linear Discriminant Analysis) model on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use training set to train a model. With the model predict the classes of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. For each class calculate the precision on test set. Which class of images was the easiest\n",
    "to predict based on fMRI data? Which one was the hardest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "# <center> End of obligatory exercises </center>  \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Precision, Recall, F1-score (Bonus 1pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes accuracy can fail us if we are dealing with unbalanced datasets. If we classify images\n",
    "of cats and dogs and in out test set we have 90 % of cats, it is possible to achieve accuracy of 0.9\n",
    "by simply always answering ”cat”. One possible countermeasure is to look not at the accuracy\n",
    "of the model, but at its *precision* and *recall*.\n",
    "\n",
    "Imagine you have 2 classes: ”cat” and ”dog”. Precision is calculated for each class separately\n",
    "and shows how many of the instances the model has identified as cats are really cats. For example\n",
    "if out of 10 instances classified as ”cats” two turn out to be ”dogs” we say that precision is 0.8.\n",
    "Recall is also calculated separately for each class. It shows how many of all cats present in\n",
    "the test set your model correctly identified as such. For example if there were 100 cats and 100\n",
    "dogs and our model correctly classified only 78 cats (other 22 it guessed as dogs) we say that its recall is 0.78.\n",
    "F1 score is a convenient metric to write precision and recall as one number:  \n",
    "\n",
    "<center> $F1 = \\dfrac{2 ·precision · recall}{precision + recall}$ </center>\n",
    "\n",
    "Your task is to calculate precision, recall and f1-score for each of 9 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
