{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Introduction to Computational Neuroscience</center></h1>\n",
    "<h1><center> Practice VI: Artificial Neural Networks </center></h1>\n",
    "<center>Aqeel Labash, Daniel Majoral, Raul Vicente</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important:\n",
    "Make sure that you saved your ipynb file correctly to avoid loss of information. Please submit this **ipynb** file only (unless you have extra files then zip this file with the extra ones). Everything should be included in this file questions, answers, codes, plots, and comments about your solutions.\n",
    "\n",
    "My **Pseudonym** is: <font color='green'>[YOUR ANSWER]</font> and it took me approximately: <font color='green'>[YOUR ANSWER]</font> hours to complete the home work.\n",
    "\n",
    "The data of how long it took you to complete the home work will help us to improve the home works to be balanced.\n",
    "\n",
    "### Before you start:\n",
    "You need to install few more packages to run this excercise. So activate your environment and then run:\n",
    "\n",
    "1. Install library to show sequeice of images: `pip install JSAnimation`\n",
    "2. Installing keras:\n",
    "   1. If you a nvidia GPU (only nvidia) and you would like to see it in action (besides games ;) ):\n",
    "      1. `conda install keras-gpu`\n",
    "   2. If you don't have nvidia GPU or you want something more simple (with gpu you might need to do some extra work)\n",
    "      1. `conda install keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Introduction\n",
    "In this session we are going to have a brief look on artificial neural networks. We start with simplest artificial neuron model called perceptron. Then we will see how simple feed-forward neural networks can be thought of as universal function approximators and what their limitations are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 : Perceptron (1pt)\n",
    "![title](imgs/perceptron.png)\n",
    "<p style=\"text-align: center;\"> <b>Figure 1:</b> Simple perceptron.</p> \n",
    "\n",
    "Perceptron is the simplest artificial network model invented by Frank Rosenblatt in late 1950s. He added a learning rule to McCulloch-Pitts neuron, that allows it to learn certain functions from example inputs and outputs.\n",
    "\n",
    "Perceptron works on binary data – both its inputs $x_j$ and output $y$ are ones or zeros. Output 1 or 0 can be thought of as binary classification – whether object represented by the given input belongs to certain class or not. \n",
    "\n",
    "Perceptron’s weights $w_j$ can be any real numbers. Its prediction is calculated with following formula:\n",
    "\n",
    "$$\n",
    "y = \\left\\{\n",
    "\t\\begin{array}{l l}\n",
    "\t\t1, \\text{if } x_1 w_1 + ... + x_m w_m + b \\geq 0\\\\\n",
    "\t\t0, \\text{otherwise}\n",
    "\t\\end{array}\n",
    "\t\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Here $b$ is the bias term, that is added to the sum. In practice it is easier to just add additional input, which is always one. Then we don’t have to treat bias as something special, it is just an additional weight to learn. The learning rule for the perceptron is very simple:\n",
    "\n",
    "$$\n",
    "w_j = w_j + (t_i - y_i) x_{ij}\n",
    "$$\n",
    "\n",
    "where index $i$ denotes the $i$-th example data point. The learning rule must be applied for all data points and for each weight. You will continue updating the weights until all data points are classified correctly.\n",
    "\n",
    "It turns out, that the perceptron is always able to successfully learn a classification rule for datasets, which are *linearly separable* -- data points with label 1 and label 0 can be separated by a line (in case of two inputs), plane (in case of three inputs) or hyperplane (in case of input of any dimensionality). If the dataset is not linearly separable, perceptron will never *converge* (settle to a certain set of weight values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "from JSAnimation import IPython_display\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline\n",
    "# Keras is a library for developing neural networks.\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense\n",
    "from keras.optimizers import sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to read data sets\n",
    "def Load_data(dataset_name='dataset1.csv'):\n",
    "    ds = pd.read_csv('data/{}'.format(dataset_name),header=None)\n",
    "    ds.columns = ['x1','x2','y']\n",
    "    return ds\n",
    "\n",
    "# Function to plot data\n",
    "def Plot_perceptron(ax,X,y,weights=None,title='perceptron classification'):\n",
    "    ax.clear()\n",
    "    def scat(ax,dat,size,co,mar):\n",
    "        ax.scatter(dat[:,0], dat[:,1],s=size, c=co,marker=mar)\n",
    "    \n",
    "    if weights is None:\n",
    "        predictions=y\n",
    "    else:\n",
    "        predictions = (np.dot(X, weights)>=0)\n",
    "        # apply threshold - result is either 0 or 1\n",
    "        \n",
    "    pos_data_correct   = X[(y == 1) & (predictions == 1)]\n",
    "    pos_data_incorrect = X[(y == 1) & (predictions == 0)]\n",
    "    neg_data_correct   = X[(y == 0) & (predictions == 0)]\n",
    "    neg_data_incorrect = X[(y == 0) & (predictions == 1)]\n",
    "    plt.title(title)\n",
    "    size = 100\n",
    "    # plot positive examples classified correctly as blue crosses\n",
    "    scat(ax,pos_data_correct,size,'b','+')\n",
    "    # plot positive examples classified incorrectly as red crosses\n",
    "    scat(ax,pos_data_incorrect,size,'r','+')\n",
    "    # plot negative examples classified correctly as green circles\n",
    "    scat(ax,neg_data_correct,size,'g','o')\n",
    "    # plot negative examples classified incorrectly as red circles\n",
    "    scat(ax,neg_data_incorrect,size,'r','o')\n",
    "    ax.legend(['TP','FN','TN','FP'])\n",
    "    ax.set_xlim((-1,1))\n",
    "    ax.set_ylim((-1,1))\n",
    "    if weights is not None:\n",
    "        ax.plot([-5, 5], [-(weights[2]-5*weights[0])/weights[1],-(weights[2]+5*weights[0])/weights[1]],c='black')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task is**\n",
    "1. Fill in the perceptron learning rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_draw_it = False\n",
    "errors=[]\n",
    "def train_perceptron(name):\n",
    "    global errors,I_draw_it\n",
    "    errors=[]\n",
    "    I_draw_it = False\n",
    "    data = Load_data(name)\n",
    "    # store number of samples and weights for convenience\n",
    "    X = data.values[:,:2]\n",
    "    \n",
    "    num_samples = X.shape[0]\n",
    "    \n",
    "    # Notice here we added one for the bias.\n",
    "    num_weights = X.shape[1]+1\n",
    "    # Here we add one more column of ones (1s) to represent the bias.\n",
    "    X = np.hstack([X,np.ones((num_samples,1))])\n",
    "    \n",
    "    target = data.values[:,2]\n",
    "    fig,ax = plt.subplots()\n",
    "    Plot_perceptron(ax,X,target,title='Data set')\n",
    "    y=target\n",
    "    w = np.random.randn(num_weights)\n",
    "    \n",
    "    # Just long enough loop for most examples to converge\n",
    "    fig,ax = plt.subplots()\n",
    "    \n",
    "    def animate(k):\n",
    "        global I_draw_it\n",
    "        # plot data and classification boundary\n",
    "        # - blue crosses are correctly classified positive samples\n",
    "        # - red crosses are incorrectly classified positive samples\n",
    "        # - green circles are correctly classified negative samples\n",
    "        # - red circles are incorrectly classified negative samples\n",
    "        # - black line shows decision boundary (not always visible!)        \n",
    "        #\n",
    "\n",
    "        # calculate activations - input multiplied by weights\n",
    "        # NB! we calculate all samples at once using vectorized implementation!\n",
    "        a = np.dot(X, w)\n",
    "        # apply threshold - result is either 0 or 1\n",
    "        y = (a >= 0);\n",
    "\n",
    "        # number of errors - where result didn't match target\n",
    "        num_errors = sum(y != target)\n",
    "        \n",
    "        # collect error history\n",
    "        errors.append(num_errors)\n",
    "        # if there were no errors then stop\n",
    "        if num_errors == 0:\n",
    "            if I_draw_it:\n",
    "                return\n",
    "            else:\n",
    "                I_draw_it=True\n",
    "                print('We have {} errors'.format(num_errors))\n",
    "                Plot_perceptron(ax,X,target,weights=w);\n",
    "                return\n",
    "        else:\n",
    "            print('We have {} errors'.format(num_errors))\n",
    "            Plot_perceptron(ax,X,target,weights=w);\n",
    "            \n",
    "\n",
    "        # otherwise do the learning\n",
    "        for i in range(num_samples):\n",
    "            for j in range(num_weights):\n",
    "                #######################################\n",
    "                ######## Your code Starts Here ########\n",
    "                # TODO: fill in the perceptron learning rule! (adding learning rate is not needed)\n",
    "                \n",
    "                #w[j] = ???\n",
    "                \n",
    "                ######### Your code ends Here #########\n",
    "                #######################################\n",
    "    anim = matplotlib.animation.FuncAnimation(fig, animate,frames=21)#, interval=20, blit=True)\n",
    "    HTML(anim.to_jshtml())\n",
    "    # plot error history\n",
    "    plt.figure()\n",
    "    plt.plot(errors);\n",
    "    plt.title('Perceptron error history');\n",
    "    plt.xlabel('Iteration');\n",
    "    plt.ylabel('Number of errors');\n",
    "    plt.xticks(range(20))\n",
    "    plt.xlim((0, 20));\n",
    "    plt.ylim((0, 20));\n",
    "    return anim\n",
    "    #return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Try datasets 1,2,3,4 and report (repeat the following cell in the next 3 cells and replace the data set number)\n",
    "   1. if they are linearly separable or not. Of course, in 2D one can do this by just looking at the data, but in higher dimensional data this would not be the case.\n",
    "   2. For linearly separable datasets also add the approximate number of steps it took for the perceptron to convergence (to reach 0 errors).\n",
    "   \n",
    "**Note:** make sure you saved your report after generating the plots. Because we want to see the final image you got with the decision boundary (the black line) for all four datasets. Where **Decision boundary** is a line (plane/hyperplane) that separates the positive and negative datapoints - positives will be on one side and negatives on the other side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anim = train_perceptron('dataset1.csv')\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Your answer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "######## Your code Starts Here ########\n",
    "#DATA SET 2\n",
    "\n",
    "#anim = ????\n",
    "\n",
    "######### Your code ends Here #########\n",
    "#######################################\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Your answer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "######## Your code Starts Here ########\n",
    "#DATA SET 3\n",
    "\n",
    "#anim = ????\n",
    "\n",
    "######### Your code ends Here #########\n",
    "#######################################\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Your answer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "######## Your code Starts Here ########\n",
    "#DATA SET 4\n",
    "\n",
    "#anim = ????\n",
    "\n",
    "######### Your code ends Here #########\n",
    "#######################################\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Your answer</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Sinusoid\n",
    "## Exercise 2 : Function Approximation (1.5pt)\n",
    "Universal Approximation Theorem states that any continuous function can be approximated to any desired precision by a feed-forward network with a single hidden layer containing a finite number of non-linear neurons. In practice this theorem is of no use, because:\n",
    "  1. It states only, that these functions can be <u>represented</u> by feed-forward network with one hidden layer. The theorem doesn’t say anything about if this approximation is <u>learnable</u> - if with our current algorithms we can find the necessary weight values;\n",
    "  2. The construction used in the proof uses a huge number of neurons in the hidden layer. This would be unreasonable for any practical application;\n",
    "  3. As the theorem doesn’t consider learnability, it also doesn’t state anything about how well the networks generalizes to samples beyond its training data.\n",
    "\n",
    "Nevertheless the theorem is a nice concept to guide your thinking – if some problem can be described as a function calculating output based on several inputs, then its probably can be approximated reasonably well with an artificial neural network.\n",
    "![title](imgs/sine.png)\n",
    "\n",
    "In this task we are going to approximate sine function using a neural network. This neural network is very simple – it consists of just one input node (the $x$ value), several hidden nodes and one output node ($y = sin(x)$).\n",
    "\n",
    "Hidden nodes use sigmoid activation function to achieve non-linearity (the nodes apply sigmoid function to the weighted sum of their inputs). Output node is linear, no activation function is applied. Loss function is simply squared error:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2n}\\sum_{i=1}^{n} (target_i - predicted_i)^2\n",
    "$$\n",
    "\n",
    "It calculates average loss over all $n$ data points in the training set.\n",
    "\n",
    "For creating neural networks we are making use of **Keras** package desigiend originally by François Chollet.\n",
    "\n",
    "For this exercise you need to run the following code with different number of nodes and answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras is a library for developing neural networks.\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense\n",
    "from keras.optimizers import sgd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "from JSAnimation import IPython_display\n",
    "%matplotlib inline\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT_SINUSOID Plots sinusoid with target points and approximation.\n",
    "def Plot_sinusoid(ax,p_train_x, p_train_y, p_test_x=None, p_test_y=None,num=None):\n",
    "    \"\"\"\n",
    "    Plots sinusoid with target points and approximation.\n",
    "    \"\"\"\n",
    "    ax.clear()\n",
    "    if p_test_x is not None:\n",
    "        minx = np.min(p_test_x)\n",
    "        maxx = np.max(p_test_x)\n",
    "    else:\n",
    "        minx = np.min(p_train_x)\n",
    "        maxx = np.max(p_train_x)\n",
    "    sinx = np.arange(minx,maxx,0.1)\n",
    "    siny = np.sin(sinx)\n",
    "    ax.plot(sinx,siny,'g',label='Sinusoid function')\n",
    "    \n",
    "    ax.scatter(p_train_x, p_train_y, s=80, facecolors='none', edgecolors='r',label='Samples we use')\n",
    "    \n",
    "    if (p_test_x is not None) and (p_test_y is not None):\n",
    "        ax.plot(p_test_x,p_test_y,c='b',label='Testing {}'.format(num))\n",
    "    ax.legend()\n",
    "    ax.set_title('Sinusoid approximation');\n",
    "    ax.set_xlabel('x');\n",
    "    ax.set_ylabel('sin(x)')\n",
    "    ax.set_ylim([-2,2])\n",
    "    #return fig\n",
    "\n",
    "# Function to create the neural network.\n",
    "def Create_Neural_network(hidden_size=5,learning_rate=0.1,momentum=0.9,weights=None):\n",
    "    \n",
    "    # One feature per sample\n",
    "    x = Input((1,))\n",
    "\n",
    "    # 5 hidden nodes with sigmoid as activation function\n",
    "    h = Dense(hidden_size,activation='sigmoid',bias_initializer='random_uniform')(x)\n",
    "\n",
    "    # One output with linear activation function.\n",
    "    out = Dense(1,activation='linear',bias_initializer='random_uniform')(h)\n",
    "\n",
    "    # Define a model\n",
    "    model = Model(inputs=[x],outputs=[out])\n",
    "\n",
    "    # Stochastic gradient descent with the specificed\n",
    "    # learning rate and momentum.\n",
    "    optimizer = sgd(lr=learning_rate,momentum=momentum)\n",
    "\n",
    "    # Compile the model to generate the computation graph\n",
    "    model.compile(optimizer=optimizer,loss='mse')\n",
    "    if weights is not None:\n",
    "        model.set_weights(weights)\n",
    "        \n",
    "    return model\n",
    "\n",
    "def Plot_sinsuide_Component(model):\n",
    "    \"\"\"\n",
    "    Plots contributions of each hidden node.\n",
    "    Notice that we are not taking bias into account.\n",
    "    \"\"\"\n",
    "    test_x = np.arange(-4*np.pi,4*np.pi,0.1)\n",
    "    # Get the input layer\n",
    "    x = model.get_layer(index=0)\n",
    "    # Get the hidden layer\n",
    "    h = model.get_layer(index=1)\n",
    "    # Create new model where the hidden layer is the output\n",
    "    nmodel = Model(inputs=x.input,outputs=h(x.input))\n",
    "    values = nmodel.predict(test_x)\n",
    "    \n",
    "    # multiply the hidden nodes with the weights for the output node.\n",
    "    values = values * model.get_weights()[2].T\n",
    "    # add the bias for the output\n",
    "    values += model.get_weights()[3][0]\n",
    "    #Notice that we didn't sum, if we some we will get exactly the expected output.\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    for i in range(values.shape[1]):\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(test_x,values[:,i],label='node: {}'.format(i+1))\n",
    "\n",
    "    plt.title('Hidden nodes contributions')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('sin(x)')\n",
    "    plt.legend()\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(test_x,values.sum(axis=1)+model.get_weights()[3])\n",
    "    plt.title('Sum of hidden node contributions (+ bias)')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('sin(x)')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def sinusoid(hidden_size=5,learning_rate=0.1,momentum=0.9,ns=20,weights=None):\n",
    "    print(\"this function will take around 20s depending on your computer specs\")\n",
    "    # Create dataset\n",
    "    range_start = -2*np.pi;\n",
    "    range_end = 2*np.pi;\n",
    "    num_samples = ns;\n",
    "    step = (range_end - range_start) / (num_samples - 1);\n",
    "    # define x, calculate y and plot sinusoid\n",
    "    # - green line is sinusoid function\n",
    "    # - red circles are samples we use\n",
    "    train_x = np.arange(range_start,range_end,step)\n",
    "    # notice that we are adding some noise to train_y\n",
    "    train_y = np.sin(train_x) + 0.1 * np.random.randn(train_x.shape[0]);\n",
    "    \n",
    "    fig,ax = plt.subplots()\n",
    "    Plot_sinusoid(ax,train_x, train_y)\n",
    "    \n",
    "    model = Create_Neural_network(hidden_size=hidden_size,learning_rate=learning_rate,momentum=momentum,weights=weights)\n",
    "    # loop to animate learning\n",
    "    \n",
    "    def animate(k):\n",
    "        global loss\n",
    "        if k==0:\n",
    "            return\n",
    "        # set up training parameters\n",
    "        # Entire dataset is a batch\n",
    "        batchsize = num_samples;\n",
    "\n",
    "        # Initially train shorter time to make animation more interesting\n",
    "        #numepochs = min(k * 10,200);                  \n",
    "        numepochs = 10\n",
    "        # randomize the order of samples\n",
    "        size=train_y.shape[0]\n",
    "        index = np.random.choice(np.arange(size),size=size,replace=False)\n",
    "\n",
    "        # train the network\n",
    "        history = model.fit(train_x[index], train_y[index],epochs=numepochs,verbose=0)\n",
    "        loss = np.concatenate([loss,history.history['loss']])\n",
    "\n",
    "        # use fine-grained range for testing\n",
    "        test_x = np.arange(-4*np.pi,4*np.pi,0.1)\n",
    "        # calculate predictions on testing range - we willnot use this actually\n",
    "        test_y = model.predict(test_x)\n",
    "\n",
    "        # plot training examples and approximation\n",
    "        Plot_sinusoid(ax,train_x, train_y, test_x, test_y,num=k)\n",
    "    \n",
    "    anim = matplotlib.animation.FuncAnimation(fig, animate,frames=21)\n",
    "    #Force execution before plotting the loss\n",
    "    HTML(anim.to_jshtml())\n",
    "    \n",
    "    # plot loss\n",
    "    plt.figure()\n",
    "    plt.plot(loss)\n",
    "    plt.title('Evolution of loss during learning')\n",
    "    plt.xlabel('Training Epoch nr')\n",
    "    plt.ylabel('Loss on training set')\n",
    "    plt.ylim((0, np.max(loss)))\n",
    "    \n",
    "    ###########################\n",
    "    ######## Attention ########\n",
    "    # You might want to uncomment the following line for Excercise 2-2\n",
    "    \n",
    "    #Plot_sinsuide_Component(model=model)\n",
    "    ###########################\n",
    "    ###########################\n",
    "    return anim\n",
    "    \n",
    "\n",
    "\n",
    "def minimal_sinusoid(hidden_size=5,learning_rate=0.1,momentum=0.9,ns=20):\n",
    "    model = Create_Neural_network(hidden_size=hidden_size)\n",
    "    weights= model.get_weights()\n",
    "    weights[3][0]=-5 # bias of the output\n",
    "\n",
    "    # first sinusoid\n",
    "    weights[2][0,0]=2 # steepness of first sinusoid\n",
    "    weights[1][0]=-7 # shift on x axis\n",
    "    weights[0][0,0]=1 #+1/-1 for the increasing/decreasing sinusoid\n",
    "\n",
    "    # second sinusoid\n",
    "    weights[2][1,0]=2\n",
    "    weights[1][1]=7\n",
    "    weights[0][0,1]=1\n",
    "\n",
    "    # 3rd\n",
    "    weights[2][2,0]= 2;\n",
    "    weights[1][2]= 0;\n",
    "    weights[0][0,2]= 1;\n",
    "\n",
    "    #4\n",
    "    weights[2][3,0]= 2;\n",
    "    weights[1][3]= 3;\n",
    "    weights[0][0,3]= -1;\n",
    "    #5\n",
    "\n",
    "    weights[2][4,0]= 2;\n",
    "    weights[1][4]= -3;\n",
    "    weights[0][0,4]= -1;\n",
    "\n",
    "    return sinusoid(hidden_size=hidden_size,learning_rate=learning_rate,momentum=momentum,ns=ns,weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First run the **sinusoid** function exactly as it is. The script creates a neural network that tries to approximate the sine function between $-2\\pi\\ and\\ 2\\pi$, based on 20 training data points. The neural network has 5 hidden nodes. All the weights and biases are initialized with random values. It would be best if you understand more or less what the code does (read the comments). \n",
    "   1. Notice that the script will generate a plot that shows the true shape of the sine, the training data points, and the current approximation made by the network. This plot is updated as we pass over the training data and learn the weights. After the learning has finished another plot appears, summarizing how the loss decreased during training. Add the two plots to the report. Answer the following questions:\n",
    "   2. Does the neural network represent the function well within the range $-2\\pi\\ to\\ 2\\pi$? If not, how many of the curves of the sine does the network manage to capture well? (In some cases it might capture the shape well, but if you run it many times it mostly shouldn't) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss = np.array([])  \n",
    "#######################################\n",
    "######## Your code Starts Here ########\n",
    "\n",
    "#anim = ???\n",
    "\n",
    "######### Your code ends Here #########\n",
    "#######################################\n",
    "print(loss[-1])\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Your answer</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now run the function **minimal_sinusoid**. In this script we also have a network of 5 nodes, but the weights and biases are not initialized randomly, but with useful values given by me (not learned by an algorithm). In this case you should see that the network learns to capture the shape of the sine really well. As you can see - the function can be represented well by a 5-node network, but the learning algorithm we used in Q1 did not allow us to find this approximation. \n",
    "   1. Hypothesize why doesn't the network always learn a good solution if we start with randomly initialized weights. In here uncommenting the line  `Plot_sinsuide_Component(test_x=test_x,model=model)` might provide insights.\n",
    "   2. How good is the network at extrapolation (predicting values outside the range it had training data for)? Why is that not surprising? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss=np.array([])\n",
    "#######################################\n",
    "######## Your code Starts Here ########\n",
    "\n",
    "#anim = ???\n",
    "\n",
    "######### Your code ends Here #########\n",
    "#######################################\n",
    "print(loss[-1])\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Increase the number of hidden nodes in **sinusoid** function, by passing `hidden_nodes` parameter with the desired number.\n",
    "   1. How many hidden nodes do you need so that starting from randomly initalized parameters the learning algorithm would converge to a good representation of the function (training loss$<0.1$). Run the code couple of times for each tested number of nodes and report the result if at least one trial is good. Keep the plot with the datapoints and the approximation (don't run the cell again basically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.array([])  \n",
    "#######################################\n",
    "######## Your code Starts Here ########\n",
    "\n",
    "#anim = ???\n",
    "\n",
    "######### Your code ends Here #########\n",
    "#######################################\n",
    "print(loss[-1])\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>Your answer</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
